{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhB3iZZsWAWe"
   },
   "source": [
    "<h1 style=\"font-size:30px;\">Data Processing</h1>\n",
    "\n",
    "In this notebook, we will cover the topic of data processing which includes exploring and cleaning datasets, as well as splitting the data between train and test, in preparation for model training and testing. We will also cover the topic of data normalization, which is a preprocessing technique that is often used to scale feature data prior to training models. We will learn how to create a normalization layer in Keras that automatically normalizes feature data for this purpose.\n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_02_data_correlation_plots-scaled.jpg' width=1000 align='center'><br/>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [1 Load the Auto MPG Dataset](#1-Load-the-Auto-MPG-Dataset)\n",
    "* [2 Clean the Dataset](#2-Clean-the-Dataset)\n",
    "* [3 Explore the Dataset](#3-Explore-the-Dataset)\n",
    "* [4 Split the Dataset into Train and Test](#4-Split-the-Dataset-into-Train-and-Test)\n",
    "* [5 Check Dataset Statistics](#5-Check-Dataset-Statistics)\n",
    "* [6 Split the Features from Target Values](#6-Split-the-Features-from-Target-Values)\n",
    "* [7 Normalize the Feature Data](#7-Normalize-the-Feature-Data)\n",
    "* [8 Conclusion](#8-Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "7P56Ncfp_R8H",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epj7UUt3_R8I"
   },
   "source": [
    "## 1 Load the Auto MPG Dataset\n",
    "\n",
    "In this notebook, we will be working with the Auto MPG dataset from the UC Irvine machine learning repository [here](https://archive.ics.uci.edu/ml/datasets.php). This data set contains nearly 400 samples of automobile data from the 1970s. There are eight data fields in the dataset consisting of various attributes such as vehicle weight and horsepower, and the goal is to use these features to predict the vehicle MPG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "kvkGR5oI_R8I",
    "outputId": "831fe1c9-3434-4f50-ac32-ed3bde4d198f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset_name = 'auto-mpg.data'\n",
    "# dataset_url  = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/'\n",
    "# dataset_path = keras.utils.get_file(dataset_name, dataset_url)\n",
    "# print('dataset_path: ', dataset_path)\n",
    "\n",
    "dataset_path = './auto-mpg.data'\n",
    "\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "# Load the dataset into a Pandas data frame.\n",
    "raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values=\"?\", comment='\\t', \n",
    "                          sep=\" \", skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHeFR8ze_R8K"
   },
   "source": [
    "## 2 Clean the Dataset\n",
    "\n",
    "Most datasets require some level of pre-processing, often referred to as \"cleaning.\" For example, some fields may be missing numeric fields labelled in the dataset with various markers ('?', 'N/A', 'NaN', etc.).  To check for such a condition, we can use the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "peXxrHpT_R8K",
    "outputId": "63640d22-c76e-4e2f-f5a2-b18ef29da650",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "WXt2s0aO_R8L",
    "outputId": "8db0a7d3-1a95-4baf-fa5f-15e6b31f0495"
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "VaUQMIT9_R8L"
   },
   "outputs": [],
   "source": [
    "# Use the dropna() method to remove data samples that are not fully populated.\n",
    "dataset = dataset.dropna()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "id": "yiTQmiL1_R8M",
    "outputId": "483d9b78-cfc1-4326-d351-f437cb42abb3"
   },
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "1t7lxcFf_R8M",
    "outputId": "65aaeb9a-7eed-4275-89f2-8736fec36a87",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q59c2dGR_R8M"
   },
   "source": [
    "## 3 Explore the Dataset\n",
    "It's essential to understand the data you are working with. We can learn quite a bit about the various features in a dataset with two simple functions, as shown below. The first figure displays a correlation matrix that quantifies how correlated the various features are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "id": "ZcXhvpc8_R8M",
    "outputId": "54f38a3b-03d4-4f94-86c5-dd45fe908960",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(dataset.corr(), cmap=plt.cm.Greens, annot=True)\n",
    "plt.title('Relationship between the features of the data', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j28wXR2_R8M"
   },
   "source": [
    "Seaborn has an excellent function called `pairplot()` that produces a grid of plots so you can better visualize the correlation between features. The distributions for each feature are shown along the diagonal. The type of distributions we're using, in this case, is called `kde` for kernel density estimate. You can think of these plots as smoothed histograms based on the sample data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "id": "mlLYts8o_R8N",
    "outputId": "144f8077-4ec9-4bc4-9c12-77c44c709626",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(dataset[['MPG', 'Horsepower', 'Displacement', 'Weight']], diag_kind='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_M6KVtX_R8N"
   },
   "source": [
    "## 4 Split the Dataset into Train and Test\n",
    "\n",
    "Let's now split the dataset into test and train components which is required in order to properly train and test models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "id": "LJf_XXwa_R8N",
    "outputId": "19f9d47c-ae96-4097-c0d4-c43e25822458"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8, random_state=42)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(test_dataset.shape)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOUjobHG_R8N"
   },
   "source": [
    "## 5 Check Dataset Statistics\n",
    "\n",
    "Let's take a look at the magnitude of the various features. As you can see from the table below, the various features have a wide range of values spanning three orders of magnitude. When the feature data varies so widley, it is generally advised to scale the feature data as a pre-processing step before training a model. Scaling of the input features will be discussed further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "_EECApxc_R8N",
    "outputId": "f4dc9fb9-dce8-4603-9808-55661118401f"
   },
   "outputs": [],
   "source": [
    "dataset.describe().transpose()[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyKRgrnz_R8N"
   },
   "source": [
    "## 6 Split the Features from Target Values\n",
    "\n",
    "Since the features and the target value are contained in the same dataframe we will separate them into two dataframes to keep them isolated. This also makes it easier to manage the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "id": "YT7NXaA4_R8N"
   },
   "outputs": [],
   "source": [
    "X_train = train_dataset.copy()\n",
    "X_test  = test_dataset.copy()\n",
    "\n",
    "# Separate target values from features.\n",
    "y_train = X_train.pop('MPG')\n",
    "y_test  = X_test.pop('MPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "-60drbIN_R8N",
    "outputId": "439a49f8-e872-421a-f008-f471d67a5674",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_stats = X_train.describe().transpose()[['mean', 'std']]\n",
    "X_train_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Normalize the Feature Data\n",
    "As mentioned above, this dataset contains a wide range of feature values, and it is often recommended to scale features so that they span a similar value range. One reason this is important is that the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs. Although a model might converge without feature scaling, scaling features makes training much more stable and also facilitates the optimization process by allowing gradient descent to converge much faster. \n",
    "\n",
    "Various methods can be used to scale features, but normalization and standardization are the most commonly used methods. Normalization usually refers to min/max scaling, where each feature is scaled to the range \\[0, 1\\] as shown below for each feature ($x_i$):\n",
    "\n",
    "$$ x_i = \\frac{x_i - min_{x_i}}{max_{x_i} - min_{x_i}}$$\n",
    "\n",
    "\n",
    "Standardization (also referred to as z-score scaling) assume the original data is normally distributed and scales the feature to have zero mean and a standard deviation of 1. This is accomplished for each feature ($x_i$) by subtracting the mean of the feature data from each data point (referred to as mean subtraction) and then dividing that result by the standard deviation for the feature data as shown below:\n",
    "\n",
    "$$ x_i = \\frac{x_i - \\mu_i}{\\sigma_i}$$\n",
    "\n",
    "Thankfully, Keras now provides a normalization layer that automatically performs feature-wise normalization to the input features, so we don't need to write our own functions to manage this conversion. The normalization layer will shift and scale inputs into a distribution centred around 0 with a standard deviation of 1. The name of this layer is a little confusing since the feature scaling nomenclature described in the paragraph above is fairly well established. In Keras, normalization refers to z-score scaling of the feature data.\n",
    "\n",
    "In the code cell below, we are using Keras to create a normalization layer for a single feature. We first convert the Horsepower feature in the dataframe to a NumPy array. We then create a normalizer object with an input shape of one since this is for a single feature. Then we use the normalizer object to call the `adapt()` method, which completes the implementation of the normalization layer. We then show that we can print out the parameters associated with this layer. In the next notebook, we will learn how to add a normalization layer to a network architecture which normalizes the input features prior to processing the date through the network. \n",
    "\n",
    "**Note**: The normalization parameters (mean and standard deviation) are derived only from the **training** dataset, but will be applied to all of the data (train, validation and test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the Horsepower feature in the dataframe to a NumPy array.\n",
    "hp = np.array(X_train['Horsepower'])\n",
    "\n",
    "# Create the normalization layer (for Horsepower). \n",
    "hp_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n",
    "\n",
    "# Call the `adapt` method to apply the normalization.\n",
    "hp_normalizer.adapt(hp)\n",
    "\n",
    "print('Mean:  ', hp_normalizer.mean.numpy()[0])\n",
    "print('Std:   ', math.sqrt(hp_normalizer.variance.numpy()[0]))\n",
    "print('Count: ', hp_normalizer.count.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Conclusion\n",
    "\n",
    "In this notebook we introduced two python packages (Pandas and Seaborn) that are often used to manage and explore datasets. We also showed how to split the data into train and test components and also showed how to create a data normalization layer in Keras to pre-process the data prior to training. In the next notebook we will use this dataset to perform both linear and non-linear regression using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week_02_06_Regression_with_Keras.ipynb",
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": [],
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": [],
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
