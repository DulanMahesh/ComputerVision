{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jclHWhBe_TTh"
   },
   "source": [
    "<h1 style=\"font-size:30px;\">Linear Regression</h1>\n",
    "\n",
    "Before studying deep neural networks, we need to cover the fundamental components of a simple (linear) neural network. We'll begin with the topic of linear regression. Since linear regression can be modeled as a linear neural network, it provides an excellent running example to introduce the essential components of neural networks. Regression is a form of supervised learning which aims to model the relationship between one or more input variables (features) and a continuous (target) variable. We assume that the relationship between the input variables $x$ and the target variable $y$ can be expressed as a weighted sum of the inputs (i.e., the model is linear in the parameters). In short, linear regression aims to learn a function that maps one or more input features to a single numerical target value.\n",
    "\n",
    "We will begin by developing a mathematical model for linear regression and will demonstrate that it can be solved analytically using ordinary least squares (OLS), which has a closed-form solution called the normal equations. We will then show how this problem can be cast as a linear neural network in Keras using just a single neuron. Finally, we will demonstrate that linear regression can be used to fit a certain class of non-linear mathematical functions.\n",
    " \n",
    "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_01_linear_regression_feature_image.png' width=1000 align='center'><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJMFBTKZ_TTr"
   },
   "source": [
    "## Table of Contents\n",
    "* [1 Linear Regression Model](#1-Linear-Regression-Model)\n",
    "* [2 Ordinary Least Squares and the Normal Equations (Closed Form Solution)](#2-Ordinary-Least-Squares-and-the-Normal-Equations-(Closed-Form-Solution))\n",
    "* [3 Fitting a Straight Line using the Normal Equations](#3-Fitting-a-Straight-Line-using-the-Normal-Equations)\n",
    "* [4 Fitting a Straight Line using Keras](#4-Fitting-a-Straight-Line-using-Keras)\n",
    "* [5 Fitting Non-Linear Data using Keras](#5-Fitting-Non\\-Linear-Data-using-Keras)\n",
    "* [6 Conclusion](#6-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDMH9mGp_TTs"
   },
   "source": [
    "## 1 Linear Regression Model\n",
    "\n",
    "**Note**: The following model description is from Andrew Ng's machine learning class. This notation is commonly used in machine learning, and we are adopting it here to introduce the topic. As we transition to neural networks, some of the notation will change (for example, the parameters $\\theta$ will be referred to as weights $w$); however, the discussion below and the use of subscripts and superscripts is helpful for communicating the concepts and dimensionality of the data.\n",
    "\n",
    "Linear regression represents an important class of supervised learning problems in which one or more features are used to predict a scalar. More formally, the goal is to learn a function that maps input features to an output scalar value. The function is sometimes referred to as a **hypothesis** and for linear regression takes the general form below.\n",
    "\n",
    "$$ h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ...$$\n",
    "\n",
    "We often simplify the notation as follows:\n",
    "\n",
    "$$ y' = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ...$$\n",
    "\n",
    "Here, the $\\theta_i$'s are the parameters (also called weights) parameterizing the space of linear functions mapping from $x$ to $y$. The problem is called **regression** when the target value is a continuous variable. When the target value represents a small number of discrete values (e.g., classes), we call it a **classification** problem. We postulate a hypothesis space in both cases and use input training data (features) to learn the parameters that fit the data. In this notebook, we will restrict our attention to regression problems.\n",
    "\n",
    "In the case of a single input feature, we have the familiar form for the equation of a line where $y'$ represents the estimated value of $y$ given an input value of $x$ and the parameters $m$ abd $b$. Here, $b$ represents $\\theta_0$ and $m$ represents $\\theta_1$.\n",
    "\n",
    "$$y' = mx + b$$\n",
    "\n",
    "To simplify the general notation, we can introduce the convention of letting $x_0 = 1$ (this is the intercept term), so that:\n",
    "\n",
    "$$ h(x) = \\sum_{i=0}^{n} \\theta_ix_i = \\theta^Tx $$\n",
    "\n",
    "where, on the right-hand side above, we are viewing $\\theta$ and $x$ both as vectors, and here $n$ is the number of input variables (not counting $x_0$). This has no consequence on the results, but makes the mathematical notation more compact for the discussion that follows.\n",
    "\n",
    "Now, given a training set, how do we pick, or learn, the parameters $\\theta$s A reasonable approach is to make $h(x)$ close to $y$, at least for the training examples we have. To formalize this, we will define a function\n",
    "that measures, for each value of the $\\theta$s, how close the $h(x(i))$’s are to the corresponding $y(i)$’s. We define the loss function:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n",
    "\n",
    "We have previously shown that the above loss function can be minimized using **gradient descent**. The next section below shows that linear regression can also be solved using a closed-form analytic solution referred to as the **normal equations**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9O7jLqr_TTt"
   },
   "source": [
    "## 2 Ordinary Least Squares and the Normal Equations (Closed Form Solution)\n",
    "\n",
    "In this section, we will develop an analytical solution to linear regression and will take the opportunity to also introduce linear algebra matrix notation to keep the equations more compact. \n",
    "\n",
    "Given a training set, we define a **design matrix** $X$ that can be represented as an $m$ x $n$ matrix that contains $m$ training examples in its rows and, $n$ represents the number of features in each training example. Often times the bias (or intercept) term is added to each row in the matrix in which case the matrix, becomes as an $m$ x $(n+1)$. <br><br>\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "- \\hspace{2mm}( x^{(1)})^T \\hspace{2mm}-\\\\\n",
    "- \\hspace{2mm}( x^{(2)})^T \\hspace{2mm}-\\\\\n",
    "       \\vdots\\\\\n",
    "- \\hspace{2mm}( x^{(m)})^T \\hspace{2mm}-\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let $\\vec{y}$ be the $m$-dimensional vector that contains all the target values from the training set.<br><br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{y} = \\begin{bmatrix}\n",
    "y^{(1)}\\\\\n",
    "y^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "y^{(m)}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Now, since $h(x^{(i)}) = (x^{(i)})^T θ$, we can easily verify that:<br><br>\n",
    "\n",
    "\\begin{equation}\n",
    "X\\theta - \\vec{y} = \\begin{bmatrix}\n",
    "- \\hspace{2mm}( x^{(1)})^T\\theta \\hspace{2mm}-\\\\\n",
    "- \\hspace{2mm}( x^{(2)})^T\\theta \\hspace{2mm}-\\\\\n",
    "\\vdots\\\\\n",
    "- \\hspace{2mm}( x^{(m)})^T\\theta \\hspace{2mm}-\\\\\n",
    "\\end{bmatrix}-\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}\\\\\n",
    "y^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "y^{(m)}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "= \\begin{bmatrix}\n",
    "h_\\theta(x^{(i)}) - y^{(i)} \\\\\n",
    "\\vdots\\\\\n",
    "h_\\theta(x^{(m)}) - y^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "<br><br>\n",
    "Using the fact that for a vector $z$, we have that $z^Tz = \\sum_i z_i^2$:<br><br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{2}(X\\theta - \\vec{y})^T(X\\theta - \\vec{y}) = \\frac{1}{2}\\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 \n",
    "\\end{equation}\n",
    "\n",
    "To minimize $J$, we compute the gradient of $J$ with respect to $\\theta$ as follows:<br><br>\n",
    "\n",
    "  $$ \\nabla_\\theta J(\\theta) = \\nabla_\\theta \\frac{1}{2} (X\\theta - \\vec{y})^T(X\\theta - \\vec{y}) $$\n",
    "  \n",
    "Carrying out the above gradient requires some matrix calculus and some matrix properties, which we will not delve into, but is easily found on the internet. The above gradient simplifies to the following expression:<br>\n",
    "\n",
    " $$ = X^TX\\theta - X^T \\vec{y} $$\n",
    " \n",
    "To minimize $J$, we set its derivative to zero, and obtain the **normal equations**:<br><br>\n",
    "\n",
    " $$ X^TX\\theta = X^T \\vec{y} $$\n",
    " \n",
    "Thus, the value of $\\theta$ that minimizes $J(\\theta)$ is given in closed-form by the following equation:<br><br>\n",
    "\n",
    "$$ \\theta = (X^TX)^{-1}X^T\\vec{y} $$\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "q0IUpR2y_TTu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 7)\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "block_plot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SltPPXuo_TTu"
   },
   "source": [
    "## 3 Fitting a Straight Line using the Normal Equations\n",
    "\n",
    "We will begin by showing that the normal equations can be used to find the parameters of a straight line (slope and intercept) given a set of data points in two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6mnqORT_TTu"
   },
   "source": [
    "### 3.1 Create Convenience Functions\n",
    "\n",
    "Let's first create a convenience function to generate some linear data with a small amount of added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "OUBhthZm_TTv"
   },
   "outputs": [],
   "source": [
    "def create_linear_data(num_data=100, y_offset=0, slope=1, stddev=.3):\n",
    "\n",
    "    # Random manual seed for consistency.\n",
    "    tf.random.set_seed(42) \n",
    "\n",
    "    # Create some linear data with a small amount of noise.\n",
    "    X = 10 * tf.random.uniform(shape=[num_data])\n",
    "    y = y_offset + slope * X + tf.random.normal(stddev=stddev, shape=[num_data])\n",
    "    \n",
    "    X = tf.reshape(X, (len(X), 1))\n",
    "    y = tf.reshape(y, (len(y), 1))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "7AwbGH_Z_TTv"
   },
   "outputs": [],
   "source": [
    "def plot_data(x, y, xlim=(0,10), ylim=(0,10)):\n",
    "    plt.figure\n",
    "    plt.plot(x, y, 'b.')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y'), \n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show(block=block_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hkz8Ucsd_TTv"
   },
   "source": [
    "### 3.2 Generate Linear Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "R1QPqIMN_TTv",
    "outputId": "b4506758-8f35-4889-9276-ab3eeb6372d2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create some data.\n",
    "y_int = 3\n",
    "slope = .4\n",
    "X, y = create_linear_data(y_offset=y_int, slope=slope, stddev=0.3)\n",
    "\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBrsK-g3_TTw"
   },
   "source": [
    "### 3.3 Implement Normal Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "ez7MFM1-_TTw"
   },
   "outputs": [],
   "source": [
    "def compute_theta(X, y):\n",
    "\n",
    "    m = X.shape[0] # Number of samples.\n",
    "\n",
    "    # Concatenate a 1 to the beginning of each feature vector.\n",
    "    X = tf.concat((tf.ones((m, 1)), X), axis=1)\n",
    "    y = tf.reshape(y, (m, 1))\n",
    "\n",
    "    # Solve for theta using the Normal Equations.\n",
    "    X_T      = tf.transpose(X)\n",
    "    XT_X     = tf.tensordot(X_T, X, axes=1)\n",
    "    XT_X_inv = tf.linalg.inv(XT_X)\n",
    "    XT_y     = tf.tensordot(X_T, y, axes=1)\n",
    "    theta    = tf.tensordot(XT_X_inv, XT_y, axes=1)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqD4LKix_TTw"
   },
   "source": [
    "### 3.4 Solve for the Model Parameters (slope and y-intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "id": "8RVLjUEh_TTw",
    "outputId": "309feca7-c60f-43a1-a56b-9b759d588c73",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Actual Coefficients:\\n')\n",
    "print('Slope: ', slope)\n",
    "print('Y-Int: ', y_int)\n",
    "print('\\n')\n",
    "\n",
    "# Compute the parameters (theta) based on the closed-form solution using the Normal Equations.\n",
    "theta = compute_theta(X, y)\n",
    "\n",
    "slope = theta[0].numpy()\n",
    "y_int = theta[1].numpy()\n",
    "\n",
    "print('Predicted Coefficients:\\n')\n",
    "print('Slope: ', slope[0])\n",
    "print('Y-int: ', y_int[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTJSRCZ9_TTw"
   },
   "source": [
    "### 3.5 Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "B-R9nrtz_TTw"
   },
   "outputs": [],
   "source": [
    "def predict_y(X, theta):\n",
    "\n",
    "    X = tf.concat((tf.ones((X.shape[0], 1)), X), axis=1)\n",
    "    pred_y = tf.matmul(X, theta)\n",
    "\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "id": "BNR-8fdX_TTw",
    "outputId": "ccdba4af-a3f2-4345-dc01-790473caa1a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_y = predict_y(X, theta)\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.plot(X, pred_y, 'c-')\n",
    "plt.xlim((0, 10))\n",
    "plt.ylim((0, 10))\n",
    "plt.text(1, 7.0, 'theta[0]: Y-Int = ' + str(slope[0]), fontsize=14, family='Consolas')\n",
    "plt.text(1, 6.5, 'theta[1]: Slope = ' + str(y_int[0]), fontsize=14, family='Consolas')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldHoP5Lq_TTx"
   },
   "source": [
    "### <font color=\"CornFlowerBlue\">Discussion</font>\n",
    "\n",
    "As you can see, using the normal equations to solve linear regression problems is very simple. However, there are several reasons why this approach can be problematic. Solving this equation requires inverting a matrix which can be computationally expensive, especially for very large problems which may include thousands of features. There is also an issue associated with its stability. Suppose that the matrix is not invertible due to numerical issues? There are methods that can mitigate some numerical problems, such as using what's known as the \"pseudo\" inverse, but this is not always ideal in practice, and these are some reasons why this approach is often not practical for large problems that we typically encounter in machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f_ikNz8x1Lz"
   },
   "source": [
    "## 4 Fitting a Straight Line using Keras\n",
    "\n",
    "Let's now take a look at solving this problem using a very simple linear neural network. \n",
    "\n",
    "### 4.1 Mathematical Model\n",
    "Create a simple linear model with one parameter (the slope) to model a straight line that passes through the origin. \n",
    "\n",
    "The goal is to predict $y$ given some value of $x$. To do this, we will fit a line that goes through the data points $(x_i, y_i)$. We will simplify the problem so that the line passes through the origin. The equation for such a line is:\n",
    "\n",
    "$$\n",
    "y = mx\n",
    "$$\n",
    "\n",
    "We have a set of data points $(x_i, y_i)$, and they should all satisfy the equation above. Therefore, \n",
    "\n",
    "$$\n",
    "y_i = m x_i\n",
    "$$\n",
    "\n",
    "The model has a single parameter $m$ (the slope of the line) that we wish to compute.\n",
    "\n",
    "### 4.2 Modeling a Linear Neural Network in Keras\n",
    "We restrict the model to a straight line that passes through the origin. Notice that this is very similar to the gradient descent notebook from the previous module, but rather than implementing all the details from scratch, we will use **Keras** to perform the same task. The network diagram below represents the simplest possible neural network. It has an input layer consisting of a single feature. Technically the input layer is not counted as a layer since there are no trainable parameters associated with it. The network has just a single layer consisting of a single neuron. The single-layer IS the output layer. The neuron has a linear activation function that simply multiplies the input feature $x$ by the weight $w$. For every training sample, the predicted output $y'$ is compared to the actual value from the training data, and the loss is computed. This allows us to compute the gradient with respect to $w$ and update the weight (slope) according to an input learning rate. All of these details are taken care of by Keras once we define the network model and call a function to train the network.\n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_01_linear_regression_wx.png' width=600 align='center'><br/>\n",
    "\n",
    "The following steps summarize the workflow in Keras:\n",
    "\n",
    "1. Build/Define a network model using either the Sequential API or the Functional API in Keras.\n",
    "2. Compile the model with `model.compile()`\n",
    "3. Train the model with `model.fit()`\n",
    "4. Predict the output `model.predict()`\n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_01_keras_workflow_basic.png' width=800 align='left'><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdrI4ICb_TTx"
   },
   "source": [
    "### 4.3 Create the Keras Model\n",
    "\n",
    "The bias term for the neuron will be set to zero in this example using the option `use_bias=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "collapsed": true,
    "id": "sMvpBFj5x1L0",
    "outputId": "90bd03ca-c48b-4f5a-d322-bcb6bd76a03d"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(1,), activation=None, use_bias=False))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2Ni2_iV_TTx"
   },
   "source": [
    "### 4.4 Compile the Model\n",
    "\n",
    "Compiling the model requires you to specify the type of loss function (in this case, mean square error: 'mse') and to also specify the specific type of optimizer to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "id": "2tKfeg6R_TTy"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqpBOjSb_TTy"
   },
   "source": [
    "### 4.5 Train the Model\n",
    "\n",
    "Once the model is compiled, we can call the `fit` method to train the model. The required arguments include the training data and the number of epochs to train for. The `fit` method returns a `history` object that we can use to access the loss for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "cViRElM__TTy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "id": "Wuei3Wzt_TTy",
    "outputId": "0829b591-7a19-4fe5-ead8-f16961662f5d"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.figure(figsize=[15, 5])\n",
    "plt.plot(epochs, loss_values, \"orange\", label=\"Training loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLEzVkjex1L9"
   },
   "source": [
    "### 4.6 Predict Model Parameters and Display Results\n",
    "\n",
    "After the model has been trained, we can access the trained parameters directly from the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "81iJ82vw_TTy",
    "outputId": "bf966b95-d746-441f-aaf7-1a08de2ea058"
   },
   "outputs": [],
   "source": [
    "slope = model.layers[0].weights[0]\n",
    "print('Slope: ', slope[0].numpy())\n",
    "\n",
    "pred_y = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "id": "FJZwh_CN_TTy",
    "outputId": "7272c596-cabe-47e1-98df-d63f16a7c5ff",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'b.')\n",
    "plt.plot(X, pred_y, 'c-') \n",
    "plt.text(1, 7.0, 'Slope = ' + str(slope[0].numpy()), fontsize=14, family=\"Consolas\")\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM6Lf7Ft_TTy"
   },
   "source": [
    "### <font color=\"CornFlowerBlue\">Discussion</font>\n",
    "\n",
    "The fitted line must pass through the origin since our model for the line only includes the slope, so it cannot fit the data very well, as shown in the plot above. Next, let's add the bias term to the network model and see how much better the fitted model performs. This is easily done in Keras by passing an input flag `use_bias=True` when we define the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zinr8Ygk_TTz"
   },
   "source": [
    "### 4.7 Create the Keras Model with a Bias Term\n",
    "\n",
    "In this section, we will model the bias term as shown in the network diagram below. \n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_01_linear_regression_wx_b.png' width=600 align='center'><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "id": "3rlKFfZP_TTz",
    "outputId": "16dc1547-570b-433d-8a28-c3c1ee698837"
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(1, input_shape=(1,), activation=None, use_bias=True))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVqeHxv4_TTz"
   },
   "source": [
    "### 4.8  Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "id": "YMim4X31_TTz"
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='mse', optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQRxmKka_TTz"
   },
   "source": [
    "### 4.9 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "id": "Va3ZGXqi_TTz"
   },
   "outputs": [],
   "source": [
    "history = model2.fit(X, y, epochs=2000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "id": "qfL-QrJ8_TTz",
    "outputId": "63174cf8-c7be-4752-f3a7-a760753a5f90"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.figure(figsize=[15, 5])\n",
    "plt.plot(epochs, loss_values, \"orange\", label=\"Training loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xpfhFEj_TTz"
   },
   "source": [
    "### 4.10  Predict Model Parameters and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "id": "OlMR09yQ_TTz",
    "outputId": "10d569b9-7c19-49d9-9d89-71363eb1b44d"
   },
   "outputs": [],
   "source": [
    "slope = model2.layers[0].weights[0]\n",
    "y_int = model2.layers[0].weights[1]\n",
    "print('Slope: ', slope[0].numpy())\n",
    "print('Y-Int: ', y_int.numpy())\n",
    "\n",
    "pred_y = model2.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "id": "2h4zLfu7_TTz",
    "outputId": "fd6a1bf0-556c-4683-88a7-7f3b115349ff",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'b.')\n",
    "plt.plot(X, pred_y, 'c-')\n",
    "plt.text(1, 7.0, 'Slope = ' + str(slope[0].numpy()), fontsize=14, family=\"Consolas\")\n",
    "plt.text(1, 6.5, 'Y-Int = ' + str(y_int.numpy()),    fontsize=14, family=\"Consolas\")\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfRepY_i_TT0"
   },
   "source": [
    "## 5 Fitting Non-Linear Data using Keras\n",
    "\n",
    "In this section, we will use a single neuron to fit a non-linear mathematical function of the form below:\n",
    "\n",
    "$$ y = \\theta_0 + \\theta_1xcos(x) + \\theta_2x^2 + noise$$\n",
    "\n",
    "Notice this is a non-linear function but it is *linear* in the parameters ($\\theta_0, \\theta_1, \\theta_2$). In other words, the independent variables can be non-linear, as in the example above. The terms $xcos(x)$ and $x^2$ are the input features we will use to train the model.\n",
    "\n",
    "An example of a functional form that is **not** linear in the parameters is given below since it cannot be expressed as the weighted sum of input features.\n",
    "\n",
    "$$ y = \\theta_0 + X^{\\theta_1}cos(X + \\theta_2)$$\n",
    "\n",
    "### 5.1 Create Convenience Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "id": "sw-n1hcX_TT0"
   },
   "outputs": [],
   "source": [
    "def create_nonlinear_data(xmin=-10, xmax=10, num_data=100, theta_0=0, theta_1=.3, theta_2=.05, noise=.1):\n",
    "\n",
    "    # Random manual seed for consistency.\n",
    "    tf.random.set_seed(42) \n",
    "    \n",
    "    X = np.linspace(xmin, xmax, num=num_data)\n",
    "    y = theta_0 + theta_1*X*np.cos(X) + theta_2*X**2 + noise*np.random.normal(size=num_data)\n",
    "    \n",
    "    X = tf.reshape(X, (len(X), 1))\n",
    "    y = tf.reshape(y, (len(y), 1))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwBmoOP5_TT0"
   },
   "source": [
    "### 5.2 Generate Non-linear Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "id": "SYvxT1Z0_TT0",
    "outputId": "76e62d53-40b4-4599-a3f3-4bf095f6c0e4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create some data (non-linear function, but linear in the parameters).\n",
    "theta_0 = -3\n",
    "theta_1 = .3\n",
    "theta_2 = .05\n",
    "X, y = create_nonlinear_data(theta_0=theta_0, theta_1=theta_1, theta_2=theta_2, noise=.3)\n",
    "\n",
    "# Create two features from the input data that match the functional form of the data we generated above.\n",
    "Xf = tf.concat((X*tf.math.cos(X), X*X), axis=1)\n",
    "temp = Xf.numpy()\n",
    "print('Xf contains two features based on X: ', temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "id": "t07CNdP5_TT0",
    "outputId": "80fb8f86-5f2b-4fe3-9ad7-bcd2cbcc9bd9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_data(X, y, (-10, 10), (-10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8Ln-gnQ_TT0"
   },
   "source": [
    "### 5.3 Create the Keras Model\n",
    "\n",
    "This model has two inputs that represent $xcos(x)$ and $x^2$ with corresponding weights $\\theta_1$ ad $\\theta_2$ respectively, and also a bias term to represent $\\theta_0$. Notice that the model summary listed below has three trainable parameters ($\\theta_0, \\theta_1, \\theta_2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "id": "o_L7gAwT_TT0",
    "outputId": "8461f557-b580-49c0-be91-accd2afb891d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(1, input_shape=(2,), activation=None, use_bias=True))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_eGxILI_TT0"
   },
   "source": [
    "### 5.4 Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "id": "0BWjNiOB_TT0"
   },
   "outputs": [],
   "source": [
    "model3.compile(loss='mse', optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYgTRwyd_TT0"
   },
   "source": [
    "### 5.5 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "id": "hr7Jqywy_TT0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model3.fit(Xf, y, epochs=2000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "id": "JO_vG-I1_TT0",
    "outputId": "b0cae787-b0d4-4091-8442-623593f79946"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.figure(figsize=[15, 5])\n",
    "plt.plot(epochs, loss_values, \"orange\", label=\"Training loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7McVwpHy_TT1"
   },
   "source": [
    "### 5.6 Predict Model Parameters and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "id": "UmVxuV5P_TT1",
    "outputId": "d1310f53-9287-4354-fe6c-993108f15920",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_theta_0 = model3.layers[0].bias[0].numpy();\n",
    "temp = model3.layers[0].weights[0].numpy();\n",
    "pred_theta_1 = temp[0]\n",
    "pred_theta_2 = temp[1]\n",
    "\n",
    "print('Actual Coefficients:\\n')\n",
    "print('theta_0 = ', theta_0)\n",
    "print('theta_1 = ', theta_1)\n",
    "print('theta_2 = ', theta_2)\n",
    "print('\\n')\n",
    "print('Predicted Coefficients:\\n')\n",
    "print('theta_0 = ', pred_theta_0)\n",
    "print('theta_1 = ', pred_theta_1[0])\n",
    "print('theta_2 = ', pred_theta_2[0])\n",
    "\n",
    "pred_y = model3.predict(Xf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sh2onPt_TT1"
   },
   "source": [
    "Notice that the parameters learned by the network are very close to the actual values that were used to generate the training data. As an exercise, you could try setting the noise to zero in the `create_nonlinear_data()` function call and confirm that the learned parameters match the parameters that were used to generate the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "id": "8FIAGMOi_TT1",
    "outputId": "d80db77f-93c1-4fa3-ed3f-672c6f6686c4"
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'b.')\n",
    "plt.plot(X, pred_y, 'c-')\n",
    "plt.text(-7, 7.0, 'theta_0 = ' + str(pred_theta_0),    fontsize=14, family=\"Consolas\")\n",
    "plt.text(-7, 6.0, 'theta_1 = ' + str(pred_theta_1[0]), fontsize=14, family=\"Consolas\")\n",
    "plt.text(-7, 5.0, 'theta_2 = ' + str(pred_theta_2[0]), fontsize=14, family=\"Consolas\")\n",
    "\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_tlaqiD_TT1"
   },
   "source": [
    "## 6 Conclusion\n",
    "\n",
    "In this notebook, we introduced the concept of linear regression in a machine learning context in which one or more features are used to predict a single scalar value. We covered the mathematical model and notation on which linear regression is based. \n",
    "\n",
    "We demonstrated that linear regression has a closed-form analytic solution derived from ordinary least squares, known as the normal equations but also highlighted that it’s often not a practical approach for many real-world problems and that gradient descent is most often the preferred method.\n",
    "\n",
    "We showed that a solution to linear regression can also be viewed as a linear neural network, and we used Keras to help us model such as case. \n",
    "\n",
    "Finally, We also showed that some non-linear mathematical functions can be solved using linear regression as long as the model is linear in the parameters (i.e., the target value can be expressed as the weighted sum of the input features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "DLiptk6k_TT1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week_01_17_Linear_Regression.ipynb",
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": [],
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": [],
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
