{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhB3iZZsWAWe"
   },
   "source": [
    "<h1 style=\"font-size:30px;\">Linear Regression to Deep Learning using Keras</h1>\n",
    "\n",
    "In this notebook, we will continue with the topic of linear regression, and we will work with a dataset that contains multiple input features. We will demonstrate how linear regression can be modeled in Keras as a linear neural network with multiple input features. This will be accomplished with a single layer neural network containing just a single neuron. Next, we will explore how adding hidden layers in the network with non-linear activation functions will produce a non-linear response.  This notebook is partially based on a tutorial available on the Tensorflow website and weâ€™ve provided that link as a reference for you [here](https://www.tensorflow.org/tutorials/keras/regression).\n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_02_regression_3D_plots.jpg' width=950 align='left'><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1 Load and Pre-Process the Auto MPG Dataset](#1-Load-and-Explore-the-Auto-MPG-Dataset)\n",
    "* [2 Keras Training Workflow](#2-Keras-Training-Workflow)\n",
    "* [3 Linear Regression](#3-Linear-Regression)\n",
    "* [4 Multivariate Linear Regression](#4-Multivariate-Linear-Regression)\n",
    "* [5 Deep Learning with a Single Feature](#5-Deep-Learning-with-a-Single-Feature)\n",
    "* [6 Deep Learning with Multiple Features](#6-Deep-Learning-with-Multiple-Features)\n",
    "* [7 Comparison of Test Results](#7-Comparison-of-Test-Results)\n",
    "* [8 Conclusion](#8-Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7P56Ncfp_R8H",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 6)\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "awfXgB7oZrim"
   },
   "outputs": [],
   "source": [
    "SEED_VALUE = 42\n",
    "\n",
    "# Fix seed to make training deterministic.\n",
    "np.random.seed(SEED_VALUE)\n",
    "tf.random.set_seed(SEED_VALUE)\n",
    "\n",
    "# For GPU.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epj7UUt3_R8I"
   },
   "source": [
    "## 1 Load and Pre-Process the Auto MPG Dataset\n",
    "\n",
    "In this notebook, we will be working with the Auto MPG dataset from the UC Irvine machine learning repository [here](https://archive.ics.uci.edu/ml/datasets.php). This data set contains nearly 400 samples of automobile data from the 1970s. There are eight data fields in the dataset consisting of various attributes such as vehicle weight and horsepower, and the goal is to use these features to predict the vehicle MPG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kvkGR5oI_R8I",
    "outputId": "831fe1c9-3434-4f50-ac32-ed3bde4d198f",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './auto-mpg.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-67ed9768304f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the dataset into a Pandas data frame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values=\"?\", comment='\\t', \n\u001b[0m\u001b[1;32m     12\u001b[0m                           sep=\" \", skipinitialspace=True)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './auto-mpg.data'"
     ]
    }
   ],
   "source": [
    "# dataset_name = 'auto-mpg.data'\n",
    "# dataset_url  = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "# dataset_path = keras.utils.get_file(dataset_name, dataset_url)\n",
    "\n",
    "dataset_path = './auto-mpg.data'\n",
    "\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "# Load the dataset into a Pandas data frame.\n",
    "raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values=\"?\", comment='\\t', \n",
    "                          sep=\" \", skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N967A2FpZriq"
   },
   "source": [
    "### 1.1 Clean the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "peXxrHpT_R8K",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "VaUQMIT9_R8L"
   },
   "outputs": [],
   "source": [
    "# Use the dropna() method to remove data samples that are not fully populated.\n",
    "dataset = dataset.dropna()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_M6KVtX_R8N"
   },
   "source": [
    "### 1.2 Split the Dataset into Train and Test\n",
    "\n",
    "Let's now split the dataset into test and train components so we can train some models and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "LJf_XXwa_R8N"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8, random_state=42)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(test_dataset.shape)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyKRgrnz_R8N"
   },
   "source": [
    "### 1.3 Split the Features from Target Values\n",
    "\n",
    "Since the features and the target value are contained in the same dataframe we will separate them into two dataframes to keep them isolated. This also makes it easier to manage the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "YT7NXaA4_R8N"
   },
   "outputs": [],
   "source": [
    "X_train = train_dataset.copy()\n",
    "X_test  = test_dataset.copy()\n",
    "\n",
    "# Separate target values from features.\n",
    "y_train = X_train.pop('MPG')\n",
    "y_test  = X_test.pop('MPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoIclHyYZriv"
   },
   "source": [
    "## 2 Keras Training Workflow\n",
    "\n",
    "The diagram shown below summarizes the training workflow in Keras that is used to create a trained model. \n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_02_keras_training_workflow.png' width=600 align='left'><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMxvxI5a_R8O"
   },
   "source": [
    "## 3 Linear Regression\n",
    "\n",
    "In this section, we will use Keras to model a single neuron (with a linear activation) to perform linear regression using just a single input variable from the dataset. We will also demonstrate how to add a normalization layer to the network model. Specifically, we'll use 'Horsepower' as the input feature to predict the MPG of the vehicle. Thus, the model will consist of a straight line with two unknown coefficients (the slope and the intercept). We will train the model to determine the coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnh5i0fDZriw"
   },
   "source": [
    "### 3.1 Normalize the Feature Data\n",
    "\n",
    "Various methods can be used to scale features, but normalization and standardization are the most commonly used methods. Normalization usually refers to min/max scaling, where each feature is scaled to the range \\[0, 1\\] as shown below for each feature ($x_i$):\n",
    "\n",
    "$$ x_i = \\frac{x_i - min_{x_i}}{max_{x_i} - min_{x_i}}$$\n",
    "\n",
    "\n",
    "Standardization (also referred to as z-score scaling) assume the original data is normally distributed and scales the feature to have zero mean and a standard deviation of 1. This is accomplished for each feature ($x_i$) by subtracting the mean from each data point (referred to as mean subtraction) and then dividing that result by the standard deviation as shown below:\n",
    "\n",
    "$$ x_i = \\frac{x_i - \\mu_i}{\\sigma_i}$$\n",
    "\n",
    "As discussed in the previous notebook, Keras now provides a normalization layer that automatically performs feature-wise normalization to the input features, so we don't need to write our own functions to manage this conversion. The normalization layer will shift and scale inputs into a distribution centred around 0 with a standard deviation of 1. The name of this layer is a little confusing since the feature scaling nomenclature described in the paragraph above is fairly well established. In Keras, normalization refers to z-score scaling of the feature data.\n",
    "\n",
    "**Note**: The normalization parameters (mean and standard deviation) are derived only from the **training** dataset, but will be applied to all of the data (train, validation and test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "zcdd-fnx_R8O"
   },
   "outputs": [],
   "source": [
    "# Convert the Horsepower feature in the dataframe to a NumPy array.\n",
    "hp = np.array(X_train['Horsepower'])\n",
    "\n",
    "# Create the normalization layer (for Horsepower). \n",
    "hp_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n",
    "\n",
    "# Call the `adapt` method to apply the normalization.\n",
    "hp_normalizer.adapt(hp)\n",
    "\n",
    "print('Mean:  ', hp_normalizer.mean.numpy()[0])\n",
    "print('Std:   ', math.sqrt(hp_normalizer.variance.numpy()[0]))\n",
    "print('Count: ', hp_normalizer.count.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "557h1PKy_R8O"
   },
   "source": [
    "### 3.2 Create Keras Model\n",
    "\n",
    "Here we define the Keras model using the Sequential API. The figure below shows the model architecture. Note: The weight update process includes the bias term even though an update equations is not explciity shown in the figure.\n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_02_linear_regression_norm_wx_b.png' width=600 align='center'><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "zJTK1VF5_R8O",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the model.\n",
    "linear_1d_model = tf.keras.Sequential()\n",
    "\n",
    "# Add the normalization layer.\n",
    "linear_1d_model.add(hp_normalizer)\n",
    "\n",
    "# Add the single neuron.\n",
    "linear_1d_model.add(Dense(1, input_shape=(1,)))\n",
    "\n",
    "# Display the model summary.\n",
    "linear_1d_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9o4kW3UJ_R8O"
   },
   "source": [
    "### 3.3 Compile the Model \n",
    "\n",
    "Here we specify the loss function and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "P9RqcJhW_R8O"
   },
   "outputs": [],
   "source": [
    "linear_1d_model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "    loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69gtPR_z_R8O"
   },
   "source": [
    "### 3.4 Train the Model\n",
    "\n",
    "It's now time to train the model using the single input feature. For this dataset and the examples in this notebook, we will be using 100 epochs. Notice that we're specifying a `validation_split` of 30%, which reserves (withholds) 30% of the training samples from being used to train the model so they can be used to evaluate the model during the training process. The `fit` method returns a `history` object that we will then be used to plot the train and validation loss curves below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Zcq_FaYi_R8O"
   },
   "outputs": [],
   "source": [
    "history_linear_1d = linear_1d_model.fit(\n",
    "    X_train['Horsepower'],\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    "    validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Infg4wwn_R8O"
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history_linear_1d.history)\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "UtVSkrPN_R8O"
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(history.history['loss'], label='Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.ylim([0, 10])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [MPG]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "zkSFOaEl_R8O",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss(history_linear_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zio29OYf_R8P"
   },
   "source": [
    "### 3.5 Model Prediction\n",
    "\n",
    "Once the model has been trained, we can now use the `predict()` method to predict `MPG` given a range of values for `Horsepower`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Q7yMqONx_R8P"
   },
   "outputs": [],
   "source": [
    "# Generate feature data for Horsepower.\n",
    "x = tf.linspace(0.0, 250, 251)\n",
    "\n",
    "# Use the model to predict MPG.\n",
    "y = linear_1d_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "MkH0x3lN_R8P"
   },
   "outputs": [],
   "source": [
    "def plot_horsepower(x, y):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.scatter(X_train['Horsepower'], y_train, label='Data', color='green', alpha=0.5)\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.grid(True)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "KfD_Aeiu_R8P",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_horsepower(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "bbOEkB6lZri0"
   },
   "outputs": [],
   "source": [
    "# Non-trainable parameters.\n",
    "mean     = linear_1d_model.layers[0].get_weights()[0]\n",
    "variance = linear_1d_model.layers[0].get_weights()[1]\n",
    "num_pts  = linear_1d_model.layers[0].get_weights()[2]\n",
    "print('Horsepower Mean:  ', int(mean*1000)/1000)\n",
    "print('Horsepower Std:   ', int(math.sqrt(variance)*1000)/1000)\n",
    "print('Horsepower Pts:   ', num_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "carDzMAoZri0"
   },
   "outputs": [],
   "source": [
    "# Normalized weights.\n",
    "slope_norm = linear_1d_model.layers[1].get_weights()[0]\n",
    "y_int_norm = linear_1d_model.layers[1].get_weights()[1]\n",
    "\n",
    "# Model parameters.\n",
    "slope = slope_norm / math.sqrt(variance)\n",
    "y_int = y_int_norm - mean*slope\n",
    "print('Model Slope: ', slope[0][0])\n",
    "print('Model Y-Int: ', y_int[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLue_iCb_R8P"
   },
   "source": [
    "## 4 Multivariate Linear Regression\n",
    "\n",
    "In this section, we will extend the previous example to now use two input features. All of the processing steps will remain the same. The only differences are those related to the input shape. With two input features, our model for the response variable is now a plane rather than a line. Using the general form that we introduced in Module 1, the hypothesis (function) takes the following form with two input features:\n",
    "\n",
    "$$ h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2$$\n",
    "\n",
    "When working with neural networks, it is more common to use the notation below:\n",
    "\n",
    "$$ y' = w_1x_1 + w_2x_2 + b$$\n",
    "\n",
    "Since we have two inputs, we have two weights (one for each input) and then a single bias term associated with the single neuron. The equations above represent the equation of a plane in 3D space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUuYMisa_R8P"
   },
   "source": [
    "### 4.1 Define the Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "HcMg5z49_R8P"
   },
   "outputs": [],
   "source": [
    "hp_dp = np.array(X_train[['Horsepower', 'Displacement']])\n",
    "print(hp_dp.shape)\n",
    "\n",
    "# Set 'axis' to be the axis of the feature dimension. Since the input shape is (num_points, 2), we need to set\n",
    "# axis=1 to indicate that each feature should be normalized. Setting axis=-1 would also work in this case \n",
    "# since a -1 indicates the last axis.\n",
    "hp_dp_normalizer = layers.Normalization(input_shape=[2,], axis=1)\n",
    "hp_dp_normalizer.adapt(hp_dp)\n",
    "\n",
    "mean = hp_dp_normalizer.mean.numpy()\n",
    "var = hp_dp_normalizer.variance.numpy()\n",
    "\n",
    "std_1 = int(math.sqrt(var[0][0])*1000)/1000\n",
    "std_2 = int(math.sqrt(var[0][1])*1000)/1000\n",
    "\n",
    "print('Mean:  ', mean[0][0], mean[0][1])\n",
    "print('Std:   ', std_1, std_2)\n",
    "print('Count: ', hp_dp_normalizer.count.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVwfCi_C_R8P"
   },
   "source": [
    "### 4.2 Build the Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "giwTAnoW_R8P",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "linear_2d_model = tf.keras.Sequential()\n",
    "linear_2d_model.add(hp_dp_normalizer)\n",
    "linear_2d_model.add(Dense(1, input_shape=(2,), activation=None))\n",
    "\n",
    "linear_2d_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07x_qEh0_R8P"
   },
   "source": [
    "### 4.3 Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "a0V0k9kX_R8P"
   },
   "outputs": [],
   "source": [
    "linear_2d_model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "    loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5lKebE5_R8P"
   },
   "source": [
    "### 4.4 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "r0FRpbAw_R8P"
   },
   "outputs": [],
   "source": [
    "history_linear_2d = linear_2d_model.fit(\n",
    "    X_train[['Horsepower', 'Displacement']],\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    "    validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "uGZeBpvaZri3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss(history_linear_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FO_TySNA_R8P"
   },
   "source": [
    "### 4.5 Visualize the Fitted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "LiRxcqe4_R8Q"
   },
   "outputs": [],
   "source": [
    "hp_min = X_train.Horsepower.min()\n",
    "hp_max = X_train.Horsepower.max()\n",
    "dp_min = X_train.Displacement.min()\n",
    "dp_max = X_train.Displacement.max()\n",
    "\n",
    "x_surf, y_surf = np.meshgrid(np.linspace(hp_min, hp_max, 100), np.linspace(dp_min, dp_max, 100))\n",
    "x_grid = pd.DataFrame({'Horsepower': x_surf.ravel(), 'Displacement': y_surf.ravel()})\n",
    "\n",
    "pred_y = linear_2d_model.predict(x_grid)\n",
    "pred_y = np.array(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Wdu8Ndrb_R8Q",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(X_train['Horsepower'], X_train['Displacement'], y_train, c='green',  marker='o', alpha=0.5)\n",
    "ax.plot_surface(x_surf, y_surf, pred_y.reshape(x_surf.shape), color='blue', alpha=0.5)\n",
    "ax.set_xlabel('Horsepower')\n",
    "ax.set_ylabel('Displacement')\n",
    "ax.set_zlabel('MPG')\n",
    "ax.view_init(9, -40)\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.scatter(X_train['Horsepower'], X_train['Displacement'], y_train, c='green', marker='o', alpha=0.5)\n",
    "ax.plot_surface(x_surf, y_surf, pred_y.reshape(x_surf.shape), color='blue', alpha=0.5)\n",
    "ax.set_xlabel('Horsepower')\n",
    "ax.set_ylabel('Displacement')\n",
    "ax.set_zlabel('MPG')\n",
    "ax.view_init(9, 140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cU_DrUbW_R8Q"
   },
   "source": [
    "## 5 Deep Learning with a Single Feature\n",
    "\n",
    "We are now ready to model hidden layers in our network. Adding hidden layers with non-linear activation functions is what makes them \"Deep\" and capable of learning general non-linear functional mappings between inputs and outputs. The simplified network diagram below shows the general architecture, which consists of an input layer, two hidden layers, and the output layer. In this section, we are going to model a network like this using two input features. We will use a non-linear (`relu`) activation function in the hidden layers that will allow the network to model non-linear functions. \n",
    "\n",
    "<img src='https://learnopencv.com/wp-content/uploads/2022/01/c4_02_DL_high_level.png' width=700 align='center'><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaPoEIl6_R8Q"
   },
   "source": [
    "### 5.1 Build the Keras Model\n",
    "\n",
    "Here we are going to build a simple deep neural network with two hidden layers (each with 32 neurons). Each of the neurons in the hidden layers will use `relu` activation functions. The choice of two hidden layers and 32 neurons for each layer is rather arbitrary, but we will need at least one hidden layer to model a non-linear response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "huraqbPU_R8Q",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build the model in Keras.\n",
    "dnn_1d_model = tf.keras.Sequential()\n",
    "dnn_1d_model.add(hp_normalizer)                 # Previoulsy defined.\n",
    "dnn_1d_model.add(Dense(32, activation='relu'))\n",
    "dnn_1d_model.add(Dense(32, activation='relu'))\n",
    "dnn_1d_model.add(Dense(1, activation=None))\n",
    "dnn_1d_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u18v9o4aZri5"
   },
   "source": [
    "Display the initial values for the weights and biases in the first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "o5xGPg2FZri5"
   },
   "outputs": [],
   "source": [
    "hidden_1 = dnn_1d_model.layers[1]\n",
    "weights, biases = hidden_1.get_weights()\n",
    "print(weights)\n",
    "print('\\n')\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxHtklTR_R8Q"
   },
   "source": [
    "### 5.2 Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "WJK49V06_R8Q"
   },
   "outputs": [],
   "source": [
    "dnn_1d_model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTwnEATk_R8Q"
   },
   "source": [
    "### 5.3 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "QdYyKGhN_R8Q"
   },
   "outputs": [],
   "source": [
    "history_dnn_1d = dnn_1d_model.fit(\n",
    "    X_train['Horsepower'],\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    "    validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "tUJVLPrI_R8Q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss(history_dnn_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIuj-rVe_R8Q"
   },
   "source": [
    "### 5.4 Visualize the Fitted Model\n",
    "\n",
    "As you can see in the plot below, we now have a non-linear response from the neural network. In the next section we are going to add a second input feature so we an visualize the non-linear response in more than two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "FmP2GdaP_R8Q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(0.0, 250, 251)\n",
    "y = dnn_1d_model.predict(x)\n",
    "\n",
    "plot_horsepower(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqmzG6cD_R8Q"
   },
   "source": [
    "## 6 Deep Learning with Multiple Features\n",
    "\n",
    "We will now introduce a second feature as we did in the Linear Regression section. This time we'll be using two input features with a deep neural network so that we can demonstrate the non-linear behavior of a neural work in more than two dimensions. When we used two input variables with just a single neuron (linear regression) the response function was a 2D plane in 3D space. This time we expect a non-linear surface in 3D space that better fits the data. Let's continue on and see!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZhXv3Kw_R8Q"
   },
   "source": [
    "### 6.1 Build the Keras Model\n",
    "\n",
    "The model below is the same as the one in the previous section with the exception that we are using two input features instead of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "UdrP1Xdy_R8Q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnn_2d_model = tf.keras.Sequential()\n",
    "dnn_2d_model.add(hp_dp_normalizer)                # Previously defined.\n",
    "dnn_2d_model.add(Dense(32, activation='relu'))\n",
    "dnn_2d_model.add(Dense(32, activation='relu'))\n",
    "dnn_2d_model.add(Dense(1, activation=None))\n",
    "dnn_2d_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B-K-Wr5_R8Q"
   },
   "source": [
    "### 6.2 Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "a0ldTL7h_R8Q"
   },
   "outputs": [],
   "source": [
    "dnn_2d_model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnp2xRh__R8R"
   },
   "source": [
    "### 6.3 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5faBbm-r_R8R"
   },
   "outputs": [],
   "source": [
    "history_dnn_2d = dnn_2d_model.fit(\n",
    "    X_train[['Horsepower', 'Displacement']],\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    "    validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "pJdAXepc_R8R",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss(history_dnn_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRXemaPh_R8R"
   },
   "source": [
    "### 6.4 Visualize the Fitted Model\n",
    "\n",
    "As shown in the plots below the response function from the network is a non-linear surface which does a better job of fitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "IscQsqh2_R8R"
   },
   "outputs": [],
   "source": [
    "hp_min = X_train.Horsepower.min()\n",
    "hp_max = X_train.Horsepower.max()\n",
    "dp_min = X_train.Displacement.min()\n",
    "dp_max = X_train.Displacement.max()\n",
    "\n",
    "x_surf, y_surf = np.meshgrid(np.linspace(hp_min, hp_max, 100), np.linspace(dp_min, dp_max, 100))\n",
    "x_grid = pd.DataFrame({'Horsepower': x_surf.ravel(), 'Displacement': y_surf.ravel()})\n",
    "\n",
    "pred_y = dnn_2d_model.predict(x_grid)\n",
    "pred_y = np.array(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9Xw8oLKQ_R8R"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(X_train['Horsepower'], X_train['Displacement'], y_train, c='green',  marker='o', alpha=0.5)\n",
    "ax.plot_surface(x_surf, y_surf, pred_y.reshape(x_surf.shape), color='blue', alpha=0.5)\n",
    "ax.set_xlabel('Horsepower')\n",
    "ax.set_ylabel('Displacement')\n",
    "ax.set_zlabel('MPG')\n",
    "ax.view_init(8, -40)\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.scatter(X_train['Horsepower'], X_train['Displacement'], y_train, c='green', marker='o', alpha=0.5)\n",
    "ax.plot_surface(x_surf, y_surf, pred_y.reshape(x_surf.shape), color='blue', alpha=0.5)\n",
    "ax.set_xlabel('Horsepower')\n",
    "ax.set_ylabel('Displacement')\n",
    "ax.set_zlabel('MPG')\n",
    "ax.view_init(8, 130)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrR7HPM5_R8R"
   },
   "source": [
    "## 7 Model Comparison\n",
    "\n",
    "Let's now compare each of the four models that we developed. We can quickly evaluate the models on the test data using `model.evaluate()`. This function will return the mean absolute error associated with the test data for each model. As expected, the test error for each model is progressively lower as we introduce additional features and non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWHSl5OW_R8R"
   },
   "source": [
    "### 7.1 Compare Training Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "bJINhHbSZri9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 13))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(history_linear_1d.history['loss'], label='Loss')\n",
    "plt.plot(history_linear_1d.history['val_loss'], label='Val Loss')\n",
    "plt.ylim([0, 10])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error [MPG]')\n",
    "plt.title('Linear 1D')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(history_linear_2d.history['loss'], label='Loss')\n",
    "plt.plot(history_linear_2d.history['val_loss'], label='Val Loss')\n",
    "plt.ylim([0, 10])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error [MPG]')\n",
    "plt.title('Linear 2D')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(history_dnn_1d.history['loss'], label='Loss')\n",
    "plt.plot(history_dnn_1d.history['val_loss'], label='Val Loss')\n",
    "plt.ylim([0, 10])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error [MPG]')\n",
    "plt.title('DNN 1D')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(history_dnn_2d.history['loss'], label='Loss')\n",
    "plt.plot(history_dnn_2d.history['val_loss'], label='Val Loss')\n",
    "plt.ylim([0, 10])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error [MPG]')\n",
    "plt.title('DNN 2D')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IXVskb5Zri9"
   },
   "source": [
    "### 7.2 Model Evaluation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "z82QBGZT_R8R"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store test results.\n",
    "test_results = {}\n",
    "\n",
    "# Linear regression test results.\n",
    "test_results['linear_1d_model'] = linear_1d_model.evaluate(X_test['Horsepower'], y_test, verbose=0)\n",
    "test_results['linear_2d_model'] = linear_2d_model.evaluate(X_test[['Horsepower', 'Displacement']], \n",
    "                                                            y_test, verbose=0)\n",
    "# Deep learning regression test results.\n",
    "test_results['dnn_1d_model']    = dnn_1d_model.evaluate(X_test['Horsepower'], y_test, verbose=0)\n",
    "test_results['dnn_2d_model']    = dnn_2d_model.evaluate(X_test[['Horsepower', 'Displacement']], \n",
    "                                                            y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "bH0eMDEj_R8R",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(test_results, index=['Mean Absolute Error [MPG]']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhjyXkn1_R8R"
   },
   "source": [
    "### 7.3 Visualizing Model Predictions \n",
    "\n",
    "Let's now take a look at how well the final model (`dnn_2d_model`) predicts vehicle MPG based on two input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "aHn1rvhZ_R8R",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_predictions = dnn_2d_model.predict(X_train[['Horsepower', 'Displacement']]).flatten()\n",
    "test_predictions = dnn_2d_model.predict(X_test[['Horsepower', 'Displacement']]).flatten()\n",
    "\n",
    "plt.scatter(y_train, train_predictions, c='lightgray')\n",
    "plt.scatter(y_test, test_predictions, c='green', alpha=.5)\n",
    "plt.title('Comparison of Test Predictions (green) to Fitted Model and Training Points (grey)')\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "plt.legend(['Train', 'Test'])\n",
    "lims = [0, 50]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "plt.grid('on')\n",
    "plt.plot(lims, lims, c='lightgray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLqqgTQwZri-"
   },
   "source": [
    "We can also compute the Mean Absolute Error (MAE) manually from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "1-ioQH5sZri-"
   },
   "outputs": [],
   "source": [
    "test_error = test_predictions - y_test\n",
    "\n",
    "mae = sum(abs(test_error))/len(y_test)\n",
    "print('MAE for dnn_2d_model: ', int(mae*10000)/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEYJv8bPZri-"
   },
   "source": [
    "### 7.4 Test Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "UOZQudTa_R8R"
   },
   "outputs": [],
   "source": [
    "plt.hist(test_error, bins=40, color='green', alpha=.5)\n",
    "plt.xlabel('Prediction Error [MPG]')\n",
    "plt.grid('on')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rEZIJc1_R8R"
   },
   "source": [
    "## 8 Conclusion\n",
    "\n",
    "In this first part of this notebook, we learned how to model linear regression using a neuron (with a linear activation function), which can be thought of as a simple linear neural network with a single layer (the output layer). We showed that the response from the network is a linear function. We looked at two examples with a single input feature and two input features and showed that these correspond to response functions of a straight line and 2D plane, respectively. In higher dimensions (when the number of features exceeds 2), the response from the linear regression model becomes a hyperplane that cannot be visualized. \n",
    "\n",
    "Next, we explored the use of hidden layers and non-linear activation functions in a neural network. We showed that this type of architecture allows the network to learn non-linear mappings between the inputs and the output. We showed examples for both a single input feature and two input features to visualize the non-linear response functions. This can easily be extended to using all the features in the dataset. Although it would not be possible to visualize the response function in higher dimensions, you should expect slightly better predictions when using more features. This would be a good exercise to experiment with on your own.\n",
    "\n",
    "Much of what we have covered here also applies to classification problems as we will see later in this module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "usCwYbUyZri_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week_02_06b_Regression_with_Keras.ipynb",
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": [],
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": [],
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
